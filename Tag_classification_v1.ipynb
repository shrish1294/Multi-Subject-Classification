{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1885658,"sourceType":"datasetVersion","datasetId":1123189}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/multilabel-classification-dataset/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(df):\n    \n    def tokenize():\n        pass\n    def stemming():\n        pass\n    \n    def get_id_from_text():\n        pass\n    def get_text_from_id():\n        pass\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_tags = ['Computer Science', 'Physics', 'Mathematics','Statistics', 'Quantitative Biology', 'Quantitative Finance']\nlabel_tags\ninput_tags = ['TITLE', 'ABSTRACT']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_text as tf_text\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\nimport re\nimport string\n# Create a custom standardization function to strip HTML break tags '<br />'.\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n  return tf.strings.regex_replace(stripped_html,\n                                  '[%s]' % re.escape(string.punctuation), '')\n\n\n# Vocabulary size and number of words in a sequence.\n\ndef get_vectorize_layer(vocab_size=10000, sequence_length=100):\n    vectorize_layer = TextVectorization(\n        standardize=custom_standardization,\n        max_tokens=vocab_size,\n        output_mode='int',\n        output_sequence_length=sequence_length)\n    return vectorize_layer\n\ndef get_text_from_tokens(tensor, vocab):\n    text = \"\"\n    for i in tensor.numpy():\n        if vocab[i]:\n            text =text +  f\" {vocab[i]}\"\n\n    return text\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_labels(df, tags):\n    labels = []\n    for i, row in df.iterrows():\n        encode = [row[j] for j in tags]\n        labels.append(encode)\n    return labels\n\ndef get_train_val_input_data(df, label_cols):\n    train_data, valid_data = train_test_split(df, train_size=0.8, shuffle=True)\n    train_title_data = list(train_data['TITLE'])\n    train_abstract_data = list(train_data['ABSTRACT'])\n    valid_title_data = list(valid_data['TITLE'])\n    valid_abstract_data = list(valid_data['ABSTRACT'])\n    train_labels = get_labels(train_data, label_cols)\n    valid_labels = get_labels(valid_data, label_cols)\n    input_data = {\n        'train_data' : (train_title_data, train_abstract_data),\n        'train_label' : train_labels,\n        'valid_data' : (valid_title_data, valid_abstract_data),\n        'valid_label': valid_labels\n    }\n    return input_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset(input_data, labels, title_vectorize_layer, abstract_vectorize_layer, batch_size=32):\n    dataset = tf.data.Dataset.from_tensor_slices((input_data, labels)).map(lambda x,y : ((title_vectorize_layer(x[0]), abstract_vectorize_layer(x[1])), y)).batch(batch_size)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_vectorize_layer = get_vectorize_layer(50000)\nabstract_vectorize_layer = get_vectorize_layer(50000 ,sequence_length=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_vectorize_layer.adapt(list(df['TITLE']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abstract_vectorize_layer.adapt(list(df['ABSTRACT']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check sample representation : \ntitle_vectorize_layer(df['TITLE'][400])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = get_train_val_input_data(df, label_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = get_dataset(input_data['train_data'], input_data['train_label'], title_vectorize_layer, abstract_vectorize_layer, 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data = get_dataset(input_data['valid_data'], input_data['valid_label'], title_vectorize_layer, abstract_vectorize_layer, 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    input1, input2= tf.keras.layers.Input(shape=(100, ), name='title_input'), tf.keras.layers.Input(shape=(200, ), name='abstract_input')\n#     inputs = tf.keras.layers.Concatenate(axis=-1, name='concatenate_layer')([input1, input2])\n    title_embs = tf.keras.layers.Embedding(50000, 64, name='title_embedding_layer')(input1)\n    abstract_embs = tf.keras.layers.Embedding(50000, 64, name='abstract_embedding_layer')(input2)\n#     out = tf.keras.layers.GlobalAveragePooling1D()(embs)\n    title_lstm = tf.keras.layers.LSTM(6, return_sequences=False, return_state=False)\n    abstract_lstm = tf.keras.layers.LSTM(6, return_sequences=False, return_state=False)\n    title_whole_seq_output = title_lstm(title_embs)\n    abstract_whole_seq_output = abstract_lstm(abstract_embs)\n    merge_layer = tf.keras.layers.Add()([title_whole_seq_output, abstract_whole_seq_output])\n    outs = tf.keras.layers.Dense(6, activation='sigmoid')(merge_layer)\n    \n    return tf.keras.Model((input1, input2), outs)\n    \ndef predict(model, title, abstract):\n    title = title_vectorize_layer(title)\n#     title = tf.expand_dims(title, 0)\n    \n    abstract = abstract_vectorize_layer(abstract)\n#     abstract = tf.expand_dims(abstract, 0)\n    print(title.get_shape(), abstract.get_shape())\n    resp = model.predict((title, abstract))\n    return resp\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_data, validation_data=valid_data, epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot([i for i in range(0, history.params['epochs'])], history.history['loss'], label='train_loss')\nplt.plot([i for i in range(0, history.params['epochs'])], history.history['val_loss'], label='val_loss')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i for i in range(0, history.params['epochs'])], history.history['accuracy'], label='train_acc')\nplt.plot([i for i in range(0, history.params['epochs'])], history.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TEST DATASET PREDICTION**","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/multilabel-classification-dataset/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = predict(model, test_df['TITLE'], test_df['ABSTRACT'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame(test_preds, columns=label_tags)\nfor i in label_tags:\n    results[i] = results[i].apply(lambda x: 1 if x >=0.5 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results['TITLE'] = test_df['TITLE']\nresults['ABSTRACT'] = test_df['ABSTRACT']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}